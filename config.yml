# ROOT DIRECTORIES
DATA_DIR: ./data/
PTH_DIR: ./models/
file_name: paperswithtopic.csv


# PREPROCESS
MAX_LEN: 64
PAD: True
pre_embed: False
embed_dim: 128
drop:
  - methodology

# DATALOADER
test_size: .1
batch_size: 128


# MODEL CONFIG
model_name: bertclassification

num_class: 16
vocab_size: 30471 # as default
hidden_dim: 128 # for models
n_layers: 1
n_heads: 8

## BERTs
which_output: last_hidden_state
use_bert_embed: False

## XLMs
pad_index: 0
unk_index: 1

## CLASSIFICATION MODELS
output_attentions: False

# TRAIN
optimizer: adam
scheduler: plateau
learning_rate: .001
weight_decay: .0
loss: bce # Multi Label Margin Loss
start_epoch: 0
n_epochs: 100
early_patience: 10
save_period: 1


# MISC
seed: 42
use_saved: True
return_everything: False


# DEBUG
partial: 1